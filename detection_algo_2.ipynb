{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'person',\n",
    "    1: 'bicycle',\n",
    "    2: 'car',\n",
    "    3: 'motorcycle',\n",
    "    4: 'airplane',\n",
    "    5: 'bus',\n",
    "    6: 'train',\n",
    "    7: 'truck',\n",
    "    8: 'boat',\n",
    "    9: 'traffic light',\n",
    "    10: 'fire hydrant',\n",
    "    11: 'stop sign',\n",
    "    12: 'parking meter',\n",
    "    13: 'bench',\n",
    "    14: 'bird',\n",
    "    15: 'cat',\n",
    "    16: 'dog',\n",
    "    17: 'horse',\n",
    "    18: 'sheep',\n",
    "    19: 'cow',\n",
    "    20: 'elephant',\n",
    "    21: 'bear',\n",
    "    22: 'zebra',\n",
    "    23: 'giraffe',\n",
    "    24: 'backpack',\n",
    "    25: 'umbrella',\n",
    "    26: 'handbag',\n",
    "    27: 'tie',\n",
    "    28: 'suitcase',\n",
    "    29: 'frisbee',\n",
    "    30: 'skis',\n",
    "    31: 'snowboard',\n",
    "    32: 'sports ball',\n",
    "    33: 'kite',\n",
    "    34: 'baseball bat',\n",
    "    35: 'baseball glove',\n",
    "    36: 'skateboard',\n",
    "    37: 'surfboard',\n",
    "    38: 'tennis racket',\n",
    "    39: 'bottle',\n",
    "    40: 'wine glass',\n",
    "    41: 'cup',\n",
    "    42: 'fork',\n",
    "    43: 'knife',\n",
    "    44: 'spoon',\n",
    "    45: 'bowl',\n",
    "    46: 'banana',\n",
    "    47: 'apple',\n",
    "    48: 'sandwich',\n",
    "    49: 'orange',\n",
    "    50: 'broccoli',\n",
    "    51: 'carrot',\n",
    "    52: 'hot dog',\n",
    "    53: 'pizza',\n",
    "    54: 'donut',\n",
    "    55: 'cake',\n",
    "    56: 'chair',\n",
    "    57: 'couch',\n",
    "    58: 'potted plant',\n",
    "    59: 'bed',\n",
    "    60: 'dining table',\n",
    "    61: 'toilet',\n",
    "    62: 'TV',\n",
    "    63: 'laptop',\n",
    "    64: 'mouse',\n",
    "    65: 'remote',\n",
    "    66: 'keyboard',\n",
    "    67: 'cell phone',\n",
    "    68: 'microwave',\n",
    "    69: 'oven',\n",
    "    70: 'toaster',\n",
    "    71: 'sink',\n",
    "    72: 'refrigerator',\n",
    "    73: 'book',\n",
    "    74: 'clock',\n",
    "    75: 'vase',\n",
    "    76: 'scissors',\n",
    "    77: 'teddy bear',\n",
    "    78: 'hair drier',\n",
    "    79: 'toothbrush'\n",
    "}\n",
    " # Dictionary mapping class IDs to names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_model(model_path='yolo11n.pt'):\n",
    "    global class_labels\n",
    "    \"\"\"\n",
    "    Load YOLO model from a specified path.\n",
    "    \"\"\"\n",
    "    model = YOLO(model_path)  # Adjust model variant based on accuracy/speed tradeoff\n",
    "    class_labels = model.names  # Dictionary mapping class IDs to names\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_video_stream(source=0):\n",
    "    \"\"\"\n",
    "    Initialize video capture from the specified source.\n",
    "    \"\"\"\n",
    "    video_capture = cv2.VideoCapture(source)\n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Error: Unable to open video source.\")\n",
    "    return video_capture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Preprocess the frame to fit the YOLO input requirements.\n",
    "    Convert from BGR to RGB and resize.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    frame_resized = cv2.resize(frame_rgb, target_size)\n",
    "    return frame_resized  # No normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_object_detection(model, frame):\n",
    "    \"\"\"\n",
    "    Run object detection on a frame using the YOLO model.\n",
    "    \"\"\"\n",
    "    results = model(frame)\n",
    "    detections = results[0].boxes  # Access the boxes directly from the first result\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_detections(detections, confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Process and filter detections based on a confidence threshold.\n",
    "#     \"\"\"\n",
    "#     filtered_detections = []\n",
    "    \n",
    "#     for box in detections:\n",
    "#         conf = box.conf.item()  # Confidence score\n",
    "#         if conf >= confidence_threshold:\n",
    "#             # Extract bounding box coordinates\n",
    "#             x1, y1, x2, y2 = box.xyxy[0]  # Coordinates\n",
    "#             label = box.cls.item()  # Class label\n",
    "#             filtered_detections.append({\n",
    "#                 'box': (int(x1), int(y1), int(x2), int(y2)),\n",
    "#                 'confidence': conf,\n",
    "#                 'label': int(label)\n",
    "#             })\n",
    "    \n",
    "#     return filtered_detections\n",
    "\n",
    "# def process_detections(detections, confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Process and filter detections based on a confidence threshold.\n",
    "#     \"\"\"\n",
    "#     filtered_detections = []\n",
    "#     for det in detections:\n",
    "#         if det.conf >= confidence_threshold:\n",
    "#             x1, y1, x2, y2 = map(int, det.xyxy[0])  # Bounding box coordinates\n",
    "#             class_id = int(det.cls)  # Class ID\n",
    "#             class_name = class_labels[class_id]  # Retrieve class name\n",
    "#             confidence = float(det.conf)  # Confidence score\n",
    "#             filtered_detections.append({\n",
    "#                 'box': (x1, y1, x2, y2),\n",
    "#                 'confidence': confidence,\n",
    "#                 'label': class_name\n",
    "#             })\n",
    "#     return filtered_detections\n",
    "\n",
    "# def process_detections(detections, confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Process and filter detections based on a confidence threshold.\n",
    "#     \"\"\"\n",
    "#     filtered_detections = []\n",
    "#     for det in detections:\n",
    "#         if det.conf >= confidence_threshold:\n",
    "#             x1, y1, x2, y2 = map(int, det.xyxy[0])  # Bounding box coordinates\n",
    "#             class_id = int(det.cls)  # Class ID\n",
    "#             class_name = class_labels.get(class_id, 'Unknown')  # Retrieve class name\n",
    "#             confidence = float(det.conf)  # Confidence score\n",
    "#             filtered_detections.append({\n",
    "#                 'box': (x1, y1, x2, y2),\n",
    "#                 'confidence': confidence,\n",
    "#                 'label': class_name\n",
    "#             })\n",
    "#     return filtered_detections\n",
    "\n",
    "\n",
    "# def process_detections(results, confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Process and filter detections based on a confidence threshold.\n",
    "#     \"\"\"\n",
    "#     filtered_detections = []\n",
    "#     for box in results.boxes:\n",
    "#         if box.conf >= confidence_threshold:\n",
    "#             x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "#             class_id = int(box.cls)  # Class ID\n",
    "#             class_name = class_labels.get(class_id, 'Unknown')  # Retrieve class name\n",
    "#             confidence = float(box.conf)  # Confidence score\n",
    "#             filtered_detections.append({\n",
    "#                 'box': (x1, y1, x2, y2),\n",
    "#                 'confidence': confidence,\n",
    "#                 'label': class_name\n",
    "#             })\n",
    "#     return filtered_detections\n",
    "\n",
    "def process_detections(result, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process and filter detections based on a confidence threshold.\n",
    "    \"\"\"\n",
    "    filtered_detections = []\n",
    "    for box in result.boxes:\n",
    "        if box.conf >= confidence_threshold:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            class_id = int(box.cls)  # Class ID\n",
    "            class_name = class_labels.get(class_id, 'Unknown')  # Retrieve class name\n",
    "            confidence = float(box.conf)  # Confidence score\n",
    "            filtered_detections.append({\n",
    "                'box': (x1, y1, x2, y2),\n",
    "                'confidence': confidence,\n",
    "                'label': class_name\n",
    "            })\n",
    "    return filtered_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def annotate_frame(frame, detections):\n",
    "#     \"\"\"\n",
    "#     Draw bounding boxes and labels on the frame for each detection.\n",
    "#     \"\"\"\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = int(det['box'][0]), int(det['box'][1]), int(det['box'][2]), int(det['box'][3])\n",
    "#         label = f\"{det['label']} {det['confidence']:.2f}\"\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "#     return frame\n",
    "\n",
    "# def annotate_frame(frame, detections):\n",
    "#     \"\"\"\n",
    "#     Draw bounding boxes and labels on the frame for each detection, including proximity.\n",
    "#     \"\"\"\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['BoundingBox_X1'], det['BoundingBox_Y1'], det['BoundingBox_X2'], det['BoundingBox_Y2']\n",
    "#         label = f\"{det['Label']} {det['Confidence']:.2f}\"\n",
    "#         proximity = det['Proximity']  # Use area as proximity measure\n",
    "        \n",
    "#         # Draw bounding box and label with proximity\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, f\"Proximity: {int(proximity)}\", (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "#     return frame\n",
    "\n",
    "# def annotate_frame(frame, detections):\n",
    "#     \"\"\"\n",
    "#     Draw bounding boxes and labels on the frame for each detection, including object names and proximity.\n",
    "#     \"\"\"\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['BoundingBox_X1'], det['BoundingBox_Y1'], det['BoundingBox_X2'], det['BoundingBox_Y2']\n",
    "#         label = f\"{det['Label']} {det['Confidence']:.2f}\"  # Show object name and confidence\n",
    "#         proximity = det['Proximity']\n",
    "        \n",
    "#         # Draw bounding box and label with proximity\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "#         cv2.putText(frame, f\"Proximity: {int(proximity)}\", (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "#     return frame\n",
    "\n",
    "\n",
    "# def annotate_frame(frame, detections):\n",
    "#     \"\"\"\n",
    "#     Draw bounding boxes and labels on the frame for each detection.\n",
    "#     \"\"\"\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['box']\n",
    "#         label = f\"{det['label']} {det['confidence']:.2f}\"\n",
    "#         # Draw bounding box\n",
    "#         cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "#         # Put label text above the bounding box\n",
    "#         cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "#     return frame\n",
    " \n",
    "import cv2\n",
    "\n",
    "def annotate_frame(frame, detections):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the frame for each detection.\n",
    "    \"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        label = f\"{det['label']} {det['confidence']:.2f}\"\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        # Put label text above the bounding box\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_detection_data(detections):\n",
    "#     \"\"\"\n",
    "#     Extract metrics from detections and format for logging.\n",
    "#     \"\"\"\n",
    "#     frame_data = []\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['box']\n",
    "#         confidence = det['confidence']\n",
    "#         label = det['label']\n",
    "        \n",
    "#         # Calculate width and height of bounding box\n",
    "#         width = x2 - x1\n",
    "#         height = y2 - y1\n",
    "#         aspect_ratio = width / height if height > 0 else 0\n",
    "        \n",
    "#         # Frame-level data dictionary\n",
    "#         data = {\n",
    "#             'BoundingBox_X1': x1,\n",
    "#             'BoundingBox_Y1': y1,\n",
    "#             'BoundingBox_X2': x2,\n",
    "#             'BoundingBox_Y2': y2,\n",
    "#             'Confidence': confidence,\n",
    "#             'Label': label,\n",
    "#             'Width': width,\n",
    "#             'Height': height,\n",
    "#             'AspectRatio': aspect_ratio\n",
    "#         }\n",
    "#         frame_data.append(data)\n",
    "#     return frame_data\n",
    "\n",
    "# def extract_detection_data(detections):\n",
    "#     \"\"\"\n",
    "#     Extract metrics from detections and format for logging.\n",
    "#     \"\"\"\n",
    "#     frame_data = []\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['box']\n",
    "#         confidence = det['confidence']\n",
    "#         label = det['label']\n",
    "        \n",
    "#         # Calculate width, height, and area (proximity indicator)\n",
    "#         width = x2 - x1\n",
    "#         height = y2 - y1\n",
    "#         area = width * height  # Use bounding box area as proximity indicator\n",
    "#         aspect_ratio = width / height if height > 0 else 0\n",
    "        \n",
    "#         # Frame-level data dictionary\n",
    "#         data = {\n",
    "#             'BoundingBox_X1': x1,\n",
    "#             'BoundingBox_Y1': y1,\n",
    "#             'BoundingBox_X2': x2,\n",
    "#             'BoundingBox_Y2': y2,\n",
    "#             'Confidence': confidence,\n",
    "#             'Label': label,\n",
    "#             'Width': width,\n",
    "#             'Height': height,\n",
    "#             'AspectRatio': aspect_ratio,\n",
    "#             'Proximity': area  # Higher area -> closer proximity\n",
    "#         }\n",
    "#         frame_data.append(data)\n",
    "#     return frame_data\n",
    "\n",
    "# def extract_detection_data(detections):\n",
    "#     \"\"\"\n",
    "#     Extract metrics from detections and format for logging, including object names.\n",
    "#     \"\"\"\n",
    "#     frame_data = []\n",
    "#     for det in detections:\n",
    "#         x1, y1, x2, y2 = det['box']\n",
    "#         confidence = det['confidence']\n",
    "#         class_id = det['label']\n",
    "#         object_name = class_labels.get(class_id, 'Unknown')  # Map class ID to name\n",
    "        \n",
    "#         # Calculate width, height, and area (proximity indicator)\n",
    "#         width = x2 - x1\n",
    "#         height = y2 - y1\n",
    "#         area = width * height  # Use bounding box area as proximity indicator\n",
    "#         aspect_ratio = width / height if height > 0 else 0\n",
    "        \n",
    "#         # Frame-level data dictionary\n",
    "#         data = {\n",
    "#             'BoundingBox_X1': x1,\n",
    "#             'BoundingBox_Y1': y1,\n",
    "#             'BoundingBox_X2': x2,\n",
    "#             'BoundingBox_Y2': y2,\n",
    "#             'Confidence': confidence,\n",
    "#             'Label': object_name,  # Use object name for labeling\n",
    "#             'ClassID': class_id,\n",
    "#             'Width': width,\n",
    "#             'Height': height,\n",
    "#             'AspectRatio': aspect_ratio,\n",
    "#             'Proximity': area  # Higher area -> closer proximity\n",
    "#         }\n",
    "#         frame_data.append(data)\n",
    "#     return frame_data\n",
    "\n",
    "\n",
    "def extract_detection_data(detections):\n",
    "    \"\"\"\n",
    "    Extracts detection data for logging and analysis.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det['box']\n",
    "        label = det['label']\n",
    "        confidence = det['confidence']\n",
    "        data.append({\n",
    "            'label': label,\n",
    "            'confidence': confidence,\n",
    "            'x1': x1,\n",
    "            'y1': y1,\n",
    "            'x2': x2,\n",
    "            'y2': y2\n",
    "        })\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detection_data_to_csv(log_data, filename='detection_log.csv'):\n",
    "    \"\"\"\n",
    "    Save detection data to a CSV file for analysis.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(log_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Detection data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_average_confidence(df):\n",
    "#     \"\"\"\n",
    "#     Calculate average confidence per class from detection data.\n",
    "#     \"\"\"\n",
    "#     avg_conf = df.groupby('Label')['Confidence'].mean()\n",
    "#     print(\"Average confidence per class:\\n\", avg_conf)\n",
    "#     return avg_conf\n",
    "\n",
    "def calculate_average_confidence(detection_data):\n",
    "    \"\"\"\n",
    "    Calculates average confidence per class from detection data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(detection_data)\n",
    "    average_confidence_per_class = df.groupby('label')['confidence'].mean()\n",
    "    return average_confidence_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_real_time_detection(model_path='yolov8n.pt', video_source=0, confidence_threshold=0.3, max_duration=120000):\n",
    "#     \"\"\"\n",
    "#     Run real-time obstacle detection and log data for analysis.\n",
    "#     Stop after max_duration seconds or if 'q' is pressed.\n",
    "#     \"\"\"\n",
    "#     # Load model and initialize video stream\n",
    "#     model = load_yolo_model(model_path)\n",
    "#     video_stream = initialize_video_stream(video_source)\n",
    "    \n",
    "#     log_data = []  # List to store detection data for each frame\n",
    "#     start_time = time.time()  # Record the start time\n",
    "    \n",
    "#     while True:\n",
    "#         # Check if max duration has passed\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         if elapsed_time > max_duration:\n",
    "#             print(\"Time limit reached. Ending detection.\")\n",
    "#             break\n",
    "        \n",
    "#         # Capture frame-by-frame\n",
    "#         ret, frame = video_stream.read()\n",
    "#         if not ret:\n",
    "#             print(\"Error: Frame capture failed.\")\n",
    "#             break\n",
    "        \n",
    "#         # Preprocess and run detection\n",
    "#         preprocessed_frame = preprocess_frame(frame)\n",
    "#         detections = run_object_detection(model, preprocessed_frame)\n",
    "        \n",
    "#         # Process detections and calculate proximity\n",
    "#         filtered_detections = process_detections(detections, confidence_threshold)\n",
    "#         frame_data = extract_detection_data(filtered_detections)  # Now includes 'Proximity'\n",
    "#         log_data.extend(frame_data)  # Append data for each detected object\n",
    "        \n",
    "#         # Annotate frame with proximity\n",
    "#         annotated_frame = annotate_frame(frame, frame_data)  # Pass frame_data to include Proximity\n",
    "        \n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Real-Time Obstacle Detection', annotated_frame)\n",
    "        \n",
    "#         # Break loop on 'q' key press\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             print(\"Detection ended by user.\")\n",
    "#             break\n",
    "    \n",
    "#     # Release video capture and close windows\n",
    "#     video_stream.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "    \n",
    "#     # Save logged data to a CSV file for later analysis\n",
    "#     save_detection_data_to_csv(log_data)\n",
    "\n",
    "\n",
    "# def run_real_time_detection(confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Run real-time object detection using the YOLOv11n model.\n",
    "#     \"\"\"\n",
    "#     # Initialize video capture (0 for default camera)\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # Perform detection\n",
    "#         model = load_yolo_model()\n",
    "#         results = model(frame)\n",
    "\n",
    "#         # Process detections\n",
    "#         detections = process_detections(results, confidence_threshold)\n",
    "\n",
    "#         # Annotate frame\n",
    "#         annotated_frame = annotate_frame(frame, detections)\n",
    "\n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Real-Time Object Detection', annotated_frame)\n",
    "\n",
    "#         # Break loop on 'q' key press\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     # Release resources\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# # Run the detection\n",
    "# run_real_time_detection()\n",
    "\n",
    "# import cv2\n",
    "\n",
    "# def run_real_time_detection( confidence_threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     Run real-time object detection using the YOLOv11n model.\n",
    "#     \"\"\"\n",
    "#     # Initialize video capture (0 for default camera)\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "\n",
    "#         # Perform detection\n",
    "#         model = load_yolo_model()\n",
    "#         results = model(frame)\n",
    "\n",
    "#         # Access the first Results object\n",
    "#         result = results[0]\n",
    "\n",
    "#         # Process detections\n",
    "#         detections = process_detections(result, confidence_threshold)\n",
    "\n",
    "#         # Annotate frame\n",
    "#         annotated_frame = annotate_frame(frame, detections)\n",
    "\n",
    "#         # Display the resulting frame\n",
    "#         cv2.imshow('Real-Time Object Detection', annotated_frame)\n",
    "\n",
    "#         # Break loop on 'q' key press\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             break\n",
    "\n",
    "#     # Release resources\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "#     # Save to csv\n",
    "#     save_detection_data_to_csv(log_data)\n",
    "\n",
    "\n",
    "def run_real_time_detection( confidence_threshold=0.3, log_file='detection_log.csv'):\n",
    "    \"\"\"\n",
    "    Run real-time object detection using the YOLOv11n model with data extraction.\n",
    "    \"\"\"\n",
    "    # Initialize video capture (0 for default camera)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    all_detections = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Perform detection\n",
    "        model = load_yolo_model()\n",
    "        results = model(frame)\n",
    "        result = results[0]\n",
    "\n",
    "        # Process detections\n",
    "        detections = process_detections(result, confidence_threshold)\n",
    "\n",
    "        # Annotate frame\n",
    "        annotated_frame = annotate_frame(frame, detections)\n",
    "\n",
    "        # Extract and log detection data\n",
    "        frame_data = extract_detection_data(detections)\n",
    "        all_detections.extend(frame_data)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Real-Time Object Detection', annotated_frame)\n",
    "\n",
    "        # Break loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save detection data to CSV\n",
    "    df = pd.DataFrame(all_detections)\n",
    "    df.to_csv(log_file, index=False)\n",
    "\n",
    "    # Calculate and display average confidence per class\n",
    "    average_confidence_per_class = calculate_average_confidence(all_detections)\n",
    "    print(\"Average confidence per class:\")\n",
    "    print(average_confidence_per_class)\n",
    "\n",
    "# # Run the detection\n",
    "# run_real_time_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 269.8ms\n",
      "Speed: 6.0ms preprocess, 269.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 253.9ms\n",
      "Speed: 5.0ms preprocess, 253.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 283.8ms\n",
      "Speed: 6.0ms preprocess, 283.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 231.9ms\n",
      "Speed: 5.0ms preprocess, 231.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 266.9ms\n",
      "Speed: 7.0ms preprocess, 266.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 178.9ms\n",
      "Speed: 4.0ms preprocess, 178.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 177.9ms\n",
      "Speed: 5.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 182.9ms\n",
      "Speed: 5.0ms preprocess, 182.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 174.9ms\n",
      "Speed: 4.0ms preprocess, 174.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 2 chairs, 1 sink, 170.9ms\n",
      "Speed: 5.0ms preprocess, 170.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 177.9ms\n",
      "Speed: 4.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 178.9ms\n",
      "Speed: 4.0ms preprocess, 178.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 177.9ms\n",
      "Speed: 4.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 171.9ms\n",
      "Speed: 5.0ms preprocess, 171.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 174.9ms\n",
      "Speed: 3.0ms preprocess, 174.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 176.9ms\n",
      "Speed: 4.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 chair, 1 sink, 172.9ms\n",
      "Speed: 5.0ms preprocess, 172.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 171.9ms\n",
      "Speed: 5.0ms preprocess, 171.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 174.9ms\n",
      "Speed: 4.0ms preprocess, 174.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 176.9ms\n",
      "Speed: 4.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 170.9ms\n",
      "Speed: 4.0ms preprocess, 170.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 217.9ms\n",
      "Speed: 4.0ms preprocess, 217.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 176.9ms\n",
      "Speed: 3.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 3 chairs, 1 sink, 173.9ms\n",
      "Speed: 5.0ms preprocess, 173.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 sink, 172.9ms\n",
      "Speed: 5.0ms preprocess, 172.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 3 chairs, 1 sink, 181.9ms\n",
      "Speed: 4.0ms preprocess, 181.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 chairs, 1 dining table, 187.9ms\n",
      "Speed: 6.0ms preprocess, 187.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 1 sink, 179.9ms\n",
      "Speed: 4.0ms preprocess, 179.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 1 sink, 181.9ms\n",
      "Speed: 4.0ms preprocess, 181.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 172.9ms\n",
      "Speed: 4.0ms preprocess, 172.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 5 chairs, 174.9ms\n",
      "Speed: 4.0ms preprocess, 174.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 4 chairs, 179.9ms\n",
      "Speed: 4.0ms preprocess, 179.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 3 chairs, 174.9ms\n",
      "Speed: 4.0ms preprocess, 174.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 175.9ms\n",
      "Speed: 4.0ms preprocess, 175.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 4 chairs, 170.9ms\n",
      "Speed: 5.0ms preprocess, 170.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 bowl, 4 chairs, 175.9ms\n",
      "Speed: 4.0ms preprocess, 175.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 dog, 1 bowl, 3 chairs, 175.9ms\n",
      "Speed: 4.0ms preprocess, 175.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 dog, 1 bowl, 2 chairs, 176.9ms\n",
      "Speed: 4.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 2 chairs, 213.9ms\n",
      "Speed: 5.0ms preprocess, 213.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 198.9ms\n",
      "Speed: 5.0ms preprocess, 198.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 176.9ms\n",
      "Speed: 5.0ms preprocess, 176.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 1 chair, 172.9ms\n",
      "Speed: 3.0ms preprocess, 172.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 1 chair, 1 cell phone, 181.9ms\n",
      "Speed: 4.0ms preprocess, 181.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 175.9ms\n",
      "Speed: 4.0ms preprocess, 175.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 187.9ms\n",
      "Speed: 4.0ms preprocess, 187.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 185.9ms\n",
      "Speed: 3.0ms preprocess, 185.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 212.4ms\n",
      "Speed: 4.0ms preprocess, 212.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 187.9ms\n",
      "Speed: 6.0ms preprocess, 187.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 188.9ms\n",
      "Speed: 6.0ms preprocess, 188.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 218.9ms\n",
      "Speed: 4.0ms preprocess, 218.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 178.9ms\n",
      "Speed: 6.0ms preprocess, 178.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 189.9ms\n",
      "Speed: 4.0ms preprocess, 189.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 1 chair, 1 sink, 239.9ms\n",
      "Speed: 6.0ms preprocess, 239.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 172.9ms\n",
      "Speed: 4.0ms preprocess, 172.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 188.9ms\n",
      "Speed: 6.0ms preprocess, 188.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 199.9ms\n",
      "Speed: 7.0ms preprocess, 199.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 174.9ms\n",
      "Speed: 5.0ms preprocess, 174.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 176.9ms\n",
      "Speed: 4.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 177.9ms\n",
      "Speed: 3.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 172.9ms\n",
      "Speed: 4.0ms preprocess, 172.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 180.9ms\n",
      "Speed: 5.0ms preprocess, 180.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 176.9ms\n",
      "Speed: 5.0ms preprocess, 176.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 178.9ms\n",
      "Speed: 5.0ms preprocess, 178.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 199.9ms\n",
      "Speed: 4.0ms preprocess, 199.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 324.8ms\n",
      "Speed: 6.0ms preprocess, 324.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 cell phone, 1 sink, 184.9ms\n",
      "Speed: 5.0ms preprocess, 184.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 181.9ms\n",
      "Speed: 4.0ms preprocess, 181.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 188.9ms\n",
      "Speed: 4.0ms preprocess, 188.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 177.9ms\n",
      "Speed: 5.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 190.9ms\n",
      "Speed: 5.0ms preprocess, 190.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 178.9ms\n",
      "Speed: 4.0ms preprocess, 178.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 188.9ms\n",
      "Speed: 3.0ms preprocess, 188.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 185.9ms\n",
      "Speed: 4.0ms preprocess, 185.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 1 sink, 195.9ms\n",
      "Speed: 4.0ms preprocess, 195.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 1 chair, 180.9ms\n",
      "Speed: 5.0ms preprocess, 180.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 bowls, 2 chairs, 192.9ms\n",
      "Speed: 5.0ms preprocess, 192.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 1 cell phone, 184.9ms\n",
      "Speed: 4.0ms preprocess, 184.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 1 cell phone, 208.9ms\n",
      "Speed: 10.0ms preprocess, 208.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 241.9ms\n",
      "Speed: 5.0ms preprocess, 241.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 182.9ms\n",
      "Speed: 4.0ms preprocess, 182.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 3 chairs, 174.9ms\n",
      "Speed: 4.0ms preprocess, 174.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 182.9ms\n",
      "Speed: 4.0ms preprocess, 182.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 202.9ms\n",
      "Speed: 3.0ms preprocess, 202.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 196.9ms\n",
      "Speed: 5.0ms preprocess, 196.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 177.9ms\n",
      "Speed: 4.0ms preprocess, 177.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 3 chairs, 212.9ms\n",
      "Speed: 4.0ms preprocess, 212.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 3 chairs, 183.9ms\n",
      "Speed: 4.0ms preprocess, 183.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 2 chairs, 194.9ms\n",
      "Speed: 6.0ms preprocess, 194.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bowl, 4 chairs, 242.9ms\n",
      "Speed: 4.0ms preprocess, 242.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Average confidence per class:\n",
      "label\n",
      "bowl            0.511008\n",
      "cat             0.487751\n",
      "cell phone      0.472483\n",
      "chair           0.567136\n",
      "dining table    0.513226\n",
      "dog             0.676016\n",
      "person          0.887460\n",
      "sink            0.615458\n",
      "tv              0.322202\n",
      "Name: confidence, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# run_real_time_detection()\n",
    "run_real_time_detection(confidence_threshold=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('detection_log.csv')\n",
    "# average_confidence_per_class = calculate_average_confidence(df)\n",
    "\n",
    "# df = pd.read_csv('detection_log.csv')\n",
    "\n",
    "# Calculate average confidence per class\n",
    "# average_confidence_per_class = df.groupby('label')['confidence'].mean()\n",
    "# print(\"Average confidence per class:\\n\", average_confidence_per_class)\n",
    "\n",
    "# Calculate average confidence per object class\n",
    "# average_confidence_per_class = df.groupby('Label')['Confidence'].mean()\n",
    "# print(\"Average confidence per class:\\n\", average_confidence_per_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
