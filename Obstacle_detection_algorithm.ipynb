{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5901,
     "status": "ok",
     "timestamp": 1730473324441,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "BjHUcp4nvWFY",
    "outputId": "b9823584-81eb-4b19-f2d8-02abb4644e2e"
   },
   "outputs": [],
   "source": [
    "# Install the ultralytics package from PyPI\n",
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7085,
     "status": "ok",
     "timestamp": 1730473447479,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "kGTJVlae6PXM",
    "outputId": "e0a2370f-13a4-4e11-85de-ed1c4f4357c4"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load the YOLO model\n",
    "# model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# # Open the video file\n",
    "# video_path = \"/videos/my_video.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# print(cap)\n",
    "\n",
    "# # Loop through the video frames\n",
    "# while cap.isOpened():\n",
    "#     # Read a frame from the video\n",
    "#     success, frame = cap.read()\n",
    "\n",
    "#     if success:\n",
    "#         # Run YOLO inference on the frame\n",
    "#         results = model(frame)\n",
    "#         print(\"Successful\")\n",
    "\n",
    "#         # Visualize the results on the frame\n",
    "#         annotated_frame = results[0].plot()\n",
    "\n",
    "#         # Display the annotated frame\n",
    "#         cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
    "\n",
    "#         # Break the loop if 'q' is pressed\n",
    "#         if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#             break\n",
    "#     else:\n",
    "#         # Break the loop if the end of the video is reached\n",
    "#         print(\"Not Successful\")\n",
    "\n",
    "#         break\n",
    "\n",
    "# # Release the video capture object and close the display window\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwWyjHI_IJWZ"
   },
   "source": [
    "**Importations of Packages / Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 7888,
     "status": "ok",
     "timestamp": 1730474653225,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "r1LjxlknIZk1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tf_keras.models import load_model\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2hEBbEtGL2g"
   },
   "source": [
    " **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1730474654483,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "ISgO3dQpGdqi"
   },
   "outputs": [],
   "source": [
    "# Load images from the directory\n",
    "# directory_path = \"/content/drive/MyDrive/Digital Engineering/engg680_2024_fall/Semester_Project/image_directory\"\n",
    "# def load_images_from_directory(directory_path):\n",
    "#     images = []\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         img = cv2.imread(os.path.join(directory_path, filename))\n",
    "#         if img is not None:\n",
    "#             images.append(img)\n",
    "#     print(images)\n",
    "#     return images\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_image(image, input_size=(416, 416)):\n",
    "    image_resized = cv2.resize(image, input_size)\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return np.expand_dims(image_normalized, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 223,
     "status": "error",
     "timestamp": 1730476465187,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "pVWrz2pilIa_",
    "outputId": "9cc90bb3-46f2-439c-bbf4-bb6b1cd33a2d"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3cf6e61a0148>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# preprocess_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprocessed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprocessed_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3a7c5b72abb8>\u001b[0m in \u001b[0;36mload_images_from_directory\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "image_list = load_images_from_directory(\"\")\n",
    "# preprocess_image\n",
    "processed_images = []\n",
    "for image in image_list:\n",
    "  processed_images.append(preprocess_image(image))\n",
    "\n",
    "print(processed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7MG0IQBImZy"
   },
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1730474679987,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "C9s559tsIrlb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_yolo_model(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jIMwrmHLa6T"
   },
   "source": [
    "**Definition of Detection Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1730474683868,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "_2y6JzSPLpNd"
   },
   "outputs": [],
   "source": [
    "# Non-Maximum Suppression (NMS)\n",
    "def non_max_suppression(predictions, confidence_threshold, nms_threshold):\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "    for prediction in predictions:\n",
    "        confidence = prediction[4]\n",
    "        if confidence > confidence_threshold:\n",
    "            x, y, w, h = prediction[:4]\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(np.argmax(prediction[5:]))\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n",
    "    # nms_boxes = [(boxes[i], confidences[i], class_ids[i]) for i in indices.flatten()]\n",
    "    nms_boxes = [(boxes[i], confidences[i], class_ids[i]) for i in indices]\n",
    "    # nms_boxes = [(boxes[i[0]], confidences[i[0]], class_ids[i[0]]) for i in indices]\n",
    "\n",
    "    return nms_boxes\n",
    "\n",
    "# Bounding box decoding\n",
    "def decode_bounding_boxes(predictions, input_shape=(416, 416)):\n",
    "    boxes = []\n",
    "    for pred in predictions:\n",
    "        x, y, w, h = pred[:4]\n",
    "        box = [int(x * input_shape[0]), int(y * input_shape[1]),\n",
    "               int(w * input_shape[0]), int(h * input_shape[1])]\n",
    "        boxes.append(box)\n",
    "    return boxes\n",
    "\n",
    "# Prediction and Post-processing\n",
    "# def detect_objects(model, image, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "#     input_data = preprocess_image(image)\n",
    "#     predictions = model.predict(input_data)\n",
    "#     boxes = decode_bounding_boxes(predictions[0])\n",
    "#     results = non_max_suppression(predictions[0], confidence_threshold, nms_threshold)\n",
    "#     return results\n",
    "\n",
    "def detect_objects(model, image, confidence_threshold=0.5, nms_threshold=0.4):\n",
    "    # Preprocess the image as needed by your model\n",
    "    input_data = preprocess_image(image)\n",
    "    \n",
    "    # Ensure the model is in evaluation mode\n",
    "    # model.eval()\n",
    "    \n",
    "    # Run inference (forward pass) through the model\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        predictions = model(input_data)  # Directly use model(input_data) instead of model.predict(input_data)\n",
    "    \n",
    "    # Process the predictions\n",
    "    boxes = decode_bounding_boxes(predictions[0])\n",
    "    results = non_max_suppression(predictions[0], confidence_threshold, nms_threshold)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O53OtQ4TU7q1"
   },
   "source": [
    "**Run Object Detection on New Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1730474688528,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "1VJ-2jtkVDUj"
   },
   "outputs": [],
   "source": [
    "# Draw Bounding Boxes\n",
    "\n",
    "def draw_bounding_boxes(frame, boxes, confidences, class_ids, classes):\n",
    "    for (box, confidence, class_id) in zip(boxes, confidences, class_ids):\n",
    "        x, y, w, h = box\n",
    "        label = str(classes[class_id])\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label}: {confidence:.2f}\", (x, y - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cpzfSxpVwQm"
   },
   "source": [
    "Algorithm for Trainig Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H37XFrO9V97z"
   },
   "outputs": [],
   "source": [
    "# Compile model for training\n",
    "# def compile_model(model):\n",
    "#     model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# # Training Loop\n",
    "# def train_yolo(model, dataset, epochs):\n",
    "#     for epoch in range(epochs):\n",
    "#         for images, labels in dataset:\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 predictions = model(images)\n",
    "#                 loss = tf.reduce_mean(tf.losses.categorical_crossentropy(labels, predictions))\n",
    "#             gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#             model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#             print(f\"Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
    "#     model.save('trained_yolo_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCfvgl71YJ2Y"
   },
   "source": [
    "**Evaluation and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1730475130162,
     "user": {
      "displayName": "Okafor Patrick",
      "userId": "01648437744835055734"
     },
     "user_tz": 360
    },
    "id": "6cDK52k8YODo",
    "outputId": "b28156de-7218-4fb5-d9ad-4af60b307f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< cv2.VideoCapture 00000140B2F45890>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 34\u001b[0m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# model.eval()  #\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# compile_model(model)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# train_yolo(model, dataset, epochs=10)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, test_dataset\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m detect_in_realtime(model)\n",
      "Cell \u001b[1;32mIn[28], line 20\u001b[0m, in \u001b[0;36mdetect_in_realtime\u001b[1;34m(model, video_source)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m results \u001b[38;5;241m=\u001b[39m detect_objects(model, frame)\n\u001b[0;32m     21\u001b[0m frame \u001b[38;5;241m=\u001b[39m draw_bounding_boxes(frame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults))\n\u001b[0;32m     22\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLO Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "Cell \u001b[1;32mIn[27], line 47\u001b[0m, in \u001b[0;36mdetect_objects\u001b[1;34m(model, image, confidence_threshold, nms_threshold)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Ensure the model is in evaluation mode\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Run inference (forward pass) through the model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient computation for inference\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model(input_data)  \u001b[38;5;66;03m# Directly use model(input_data) instead of model.predict(input_data)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Process the predictions\u001b[39;00m\n\u001b[0;32m     50\u001b[0m boxes \u001b[38;5;241m=\u001b[39m decode_bounding_boxes(predictions[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "# def evaluate_model(model, test_dataset):\n",
    "#     total_iou, count = 0, 0\n",
    "#     for images, true_boxes in test_dataset:\n",
    "#         predictions = model(images)\n",
    "#         pred_boxes = decode_bounding_boxes(predictions)\n",
    "#         iou = calculate_iou(pred_boxes, true_boxes)  # Assuming a helper IoU function\n",
    "#         total_iou += iou\n",
    "#         count += 1\n",
    "#     print(f\"Mean IoU: {total_iou / count}\")\n",
    "\n",
    "# Real-time Detection from Webcam or Video\n",
    "def detect_in_realtime(model, video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    print(cap)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        results = detect_objects(model, frame)\n",
    "        frame = draw_bounding_boxes(frame, *zip(*results))\n",
    "        cv2.imshow('YOLO Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "model_path = \"yolo11n.pt\"\n",
    "model = torch.load(model_path)\n",
    "# model.eval()  #\n",
    "# compile_model(model)\n",
    "# train_yolo(model, dataset, epochs=10)\n",
    "# evaluate_model(model, test_dataset\n",
    "detect_in_realtime(model)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM094a1y0/MnDF4Y8cCKlqw",
   "mount_file_id": "1E78OfIc3X53Ay6QL4sICXdEnKGqXAuRd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
